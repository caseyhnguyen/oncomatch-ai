# OncoMatch AI - Setup & Installation Guide

Complete guide for setting up and configuring the OncoMatch AI clinical trial matching system.

---

## üìã Prerequisites

### System Requirements

- **Python**: 3.10 or higher
- **RAM**: 4GB minimum, 8GB recommended
- **Disk**: 2GB for dependencies + cache
- **Network**: Internet connection for API calls

### Required API Keys

You need **at least one** LLM provider API key:

| Provider | Cost | Speed | Recommended For |
|----------|------|-------|-----------------|
| **Google (Gemini)** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Primary** (most cost-effective) |
| OpenAI (GPT) | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Fallback/high-quality |
| Anthropic (Claude) | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Safety validation |

**Recommendation**: Start with Google API key for best cost/performance ratio.

---

## üöÄ Installation

###Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/oncomatch-ai.git
cd oncomatch-ai
```

### Step 2: Install Dependencies

```bash
# Install Python packages
pip install -r requirements.txt

# Verify installation
python -c "import openai, anthropic, google.generativeai; print('‚úÖ Dependencies installed')"
```

### Step 3: Configure Environment

Create a `.env` file in the project root:

```bash
# Copy example file
cp .env.example .env

# Edit with your API keys
nano .env  # or use your preferred editor
```

### Step 4: Add API Keys

Edit `.env` and add your API keys:

```bash
# === REQUIRED: At least ONE of these ===

# Google/Gemini (RECOMMENDED - best cost/performance)
GOOGLE_API_KEY=your_google_api_key_here

# OpenAI (optional, for fallback)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (optional, for safety validation)
ANTHROPIC_API_KEY=your_anthropic_api_key_here


# === OPTIONAL: Performance Tuning ===

# Concurrency settings
MAX_CONCURRENT_LLM=20                    # Parallel LLM calls (default: 20)
LLM_BATCH_SIZE=5                         # Trials per batch (default: 5)

# Cache settings
CACHE_TTL_HOURS=24                       # Cache expiration (default: 24)
ENABLE_LLM_CACHE=true                    # Enable caching (default: true)

# Budget controls
LLM_BUDGET_MS=15000                      # Max time per trial in ms
LLM_BUDGET_USD=0.05                      # Max cost per trial in USD
```

### Step 5: Verify Setup

```bash
# Test basic functionality
python src/match.py --patient_id P001

# Expected output: Trial matches in ~20 seconds
```

---

## üîë Getting API Keys

### Google (Gemini) - Recommended

1. Go to [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Click "Get API Key"
3. Create a new API key
4. Copy and add to `.env`: `GOOGLE_API_KEY=...`

**Cost**: ~$0.005/1k tokens (very affordable)

### OpenAI (GPT)

1. Go to [OpenAI API Keys](https://platform.openai.com/api-keys)
2. Click "Create new secret key"
3. Copy and add to `.env`: `OPENAI_API_KEY=...`

**Cost**: ~$0.03/1k tokens (moderate)

### Anthropic (Claude)

1. Go to [Anthropic Console](https://console.anthropic.com/)
2. Navigate to "API Keys"
3. Create a new key
4. Copy and add to `.env`: `ANTHROPIC_API_KEY=...`

**Cost**: ~$0.015/1k tokens (moderate)

---

## ‚öôÔ∏è Configuration

### Performance Tuning

#### For Speed (Fast Mode)
```bash
MAX_CONCURRENT_LLM=20        # Maximum parallelism
LLM_BATCH_SIZE=5             # Optimal batch size
CACHE_TTL_HOURS=24           # Long cache duration
```

**Expected**: ~18-20s per patient

#### For Quality (Thorough Mode)
```bash
MAX_CONCURRENT_LLM=10        # More careful processing
LLM_BATCH_SIZE=3             # Smaller batches
CACHE_TTL_HOURS=1            # Fresh data
```

**Expected**: ~25-30s per patient

#### For Cost Optimization
```bash
GOOGLE_API_KEY=...           # Use Gemini only
# Leave OpenAI and Anthropic blank
LLM_BUDGET_USD=0.02          # Strict cost limit
```

**Expected**: ~$0.02-0.05 per patient

### Model Selection

The system automatically selects models based on complexity. To customize:

```bash
# In src/oncomatch/llm_registry.py, adjust weights:
# - Quality weight: How much to prioritize quality
# - Cost weight: How much to prioritize cost
# - Latency weight: How much to prioritize speed
```

Default weights:
- Quality: 0.5 (50%)
- Cost: 0.3 (30%)
- Latency: 0.2 (20%)

---

## üß™ Verification

### Run Tests

```bash
# Basic functionality test
python src/match.py --patient_id P001
# Expected: ~20s, 10 trial matches

# Comprehensive test suite
python tests/test_matching.py
# Expected: ~2 min, Grade B+ (0.82)

# Judge ensemble verification
python scripts/test_judge_ensemble.py
# Expected: ~5-10 min, all 7 judges operational

# Synthetic patient demo
python scripts/demo_synthetic_evaluation.py
# Expected: ~2-3 min, 10 sample matches
```

### Expected Results

#### Successful Setup
```
‚úì All dependencies installed
‚úì API keys configured
‚úì Patient matched in ~10s
‚úì Grade: B+ (0.82)
‚úì All 7 judges operational
‚úì Latency within target (<15s)
```

#### Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| `ModuleNotFoundError` | Missing dependencies | Run `pip install -r requirements.txt` |
| `401 Unauthorized` | Invalid API key | Check `.env` file, verify key |
| `RateLimitError` | API rate limit | Add delay or use multiple providers |
| `TimeoutError` | Network issues | Check internet connection |
| `No matches found` | No trials available | Check patient data, try different patient |

---

## üìä Usage Examples

### Basic Matching

```bash
# Single patient
python src/match.py --patient_id P001

# All patients (recommended: limit trials for speed)
python src/match.py --all --max_trials 20

# With custom trial limit
python src/match.py --patient_id P002 --max_trials 10
```

### Running Evaluations

```bash
# Quick evaluation (5 patients)
python tests/test_matching.py
# Time: ~3-5 min
# Output: Grade, metrics, recommendations

# Full evaluation (1000 synthetic patients)
python scripts/run_full_evaluation.py
# Time: ~30-60 min
# Output: Comprehensive report with all metrics

# Judge ensemble test
python scripts/test_judge_ensemble.py
# Time: ~5-10 min
# Output: Judge agreement, reliability metrics
```

### Interpreting Results

#### Match Scores

- **0.9-1.0**: Excellent match - All criteria met
- **0.7-0.8**: Good match - Most criteria met
- **0.5-0.6**: Moderate match - Cancer type matches, some criteria
- **0.3-0.4**: Weak match - Limited relevance
- **0.0-0.2**: No match - Wrong cancer type or contraindications

#### Confidence Levels

- **High (0.8-1.0)**: Strong evidence, clear match
- **Medium (0.6-0.8)**: Good evidence, minor uncertainties
- **Low (0.4-0.6)**: Limited evidence, significant uncertainties

#### System Grade

- **A (0.85+)**: Excellent - Production ready
- **B+ (0.80-0.85)**: Very good - Ready for pilot
- **B (0.75-0.80)**: Good - Minor improvements needed
- **C+ (0.70-0.75)**: Satisfactory - Some improvements needed
- **C (<0.70)**: Acceptable - Significant improvements needed

**Current Grade**: B+ (0.82)

---

## üîß Troubleshooting

### Performance Issues

#### Slow Matching (>30s per patient)

```bash
# Check if caching is enabled
grep "ENABLE_LLM_CACHE" .env
# Should show: ENABLE_LLM_CACHE=true

# Reduce trial count
python src/match.py --patient_id P001 --max_trials 10

# Increase parallelism
# In .env: MAX_CONCURRENT_LLM=20
```

#### Low Cache Hit Rate

```bash
# Increase cache TTL
# In .env: CACHE_TTL_HOURS=24 or 48

# Check cache location
ls -la cache/llm_results/

# Clear cache if corrupted
rm -rf cache/llm_results/*
```

### Quality Issues

#### Low Match Scores (avg < 0.4)

The system has **automatic score normalization**:
- Monitors average scores
- If avg < 0.4, applies boost factor (1.5x max)
- Logs normalization events

Check logs for:
```
INFO:oncomatch.optimized_ranker:Normalizing scores: avg 0.350 < 0.4, boosting by 1.43x
```

#### Low Recall (<40%)

Current relevance threshold: **0.5** (includes moderate matches)

To adjust (not recommended):
```python
# In tests/test_matching.py, line 208:
if score > 0.5:  # Lower to 0.4 for more matches, raise to 0.6 for fewer
```

### API Issues

#### Rate Limits

```bash
# Use multiple providers for fallback
GOOGLE_API_KEY=...
OPENAI_API_KEY=...
ANTHROPIC_API_KEY=...

# System will automatically switch providers
```

#### Cost Concerns

```bash
# Use Gemini only (most cost-effective)
GOOGLE_API_KEY=your_key
# Leave others blank

# Set strict budget
LLM_BUDGET_USD=0.02

# Reduce trial count
python src/match.py --patient_id P001 --max_trials 10
```

---

## üìö Next Steps

### For Development

1. **Read Architecture**: [ARCHITECTURE.md](ARCHITECTURE.md)
2. **Understand Metrics**: [METRICS_GUIDE.md](METRICS_GUIDE.md)
3. **Review Models**: [MODELS_SEPTEMBER_2025.md](MODELS_SEPTEMBER_2025.md)

### For Production Deployment

1. **Scale Testing**: Test with 100+ patients
2. **Monitor Performance**: Track latency, cache hit rate, cost
3. **Tune Configuration**: Adjust based on usage patterns
4. **Set Up Logging**: Configure structured logging for audit trails
5. **Deploy Monitoring**: Use observability tools (e.g., DataDog, New Relic)

### For Evaluation

1. **Run Full Suite**: `python scripts/run_full_evaluation.py`
2. **Review Results**: Check `outputs/results/` directory
3. **Analyze Metrics**: Focus on clinical effectiveness and safety
4. **Iterate**: Adjust prompts, thresholds, or models based on results

---

## üéØ Quick Reference

### File Locations

| File | Purpose |
|------|---------|
| `.env` | API keys and configuration |
| `patients.csv` | 30 real patient profiles |
| `src/match.py` | Main entry point |
| `src/oncomatch/optimized_ranker.py` | High-performance ranker |
| `src/oncomatch/llm_providers.py` | LLM provider adapters |
| `cache/llm_results/` | LLM response cache |
| `outputs/results/` | Evaluation results |

### Common Commands

```bash
# Match single patient
python src/match.py --patient_id P001

# Run test suite
python tests/test_matching.py

# Full evaluation
python scripts/run_full_evaluation.py

# Clear cache
rm -rf cache/llm_results/*

# View logs
tail -f outputs/logs/*.log
```

### Key Metrics

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| Latency | **10.5s** | <15s | Within target |
| P95 Latency | 14.4s | <30s | Within target |
| Match Quality | 0.69 | >0.70 | Close |
| Recall@10 | 0.10 | >0.45 | Below target |
| Precision | 1.00 | >0.70 | Met |
| nDCG | 0.97 | >0.80 | Met |
| Grade | **B+ (0.82)** | B+ (0.80) | Met |

---

## ‚úÖ Setup Checklist

- [ ] Python 3.10+ installed
- [ ] Repository cloned
- [ ] Dependencies installed (`pip install -r requirements.txt`)
- [ ] `.env` file created
- [ ] At least one API key added (Gemini recommended)
- [ ] Basic test passed (`python src/match.py --patient_id P001`)
- [ ] Test suite passed (`python tests/test_matching.py`)
- [ ] Grade B+ achieved (0.80+)
- [ ] Documentation reviewed
- [ ] Ready for production pilot!

---

**Need Help?** Open a GitHub issue or check the documentation index: [INDEX.md](INDEX.md)
